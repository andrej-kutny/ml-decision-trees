{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning - Assignment 2\n",
    "\n",
    "## Decision tree induction algorithm for classification tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of the assignment is to:\n",
    "\n",
    "* Implement a decision tree induction algorithm for classification tasks.\n",
    "* Make sure it works for real valued features and nominal features (categorical features without rank, e.g., red - blue - green).\n",
    "* Test the algorithm on 3 datasets.\n",
    "\n",
    "Follow the instructions and implement what is missing to complete the assignment. Some functions have been started to help you a little bit with the inputs or outputs of the function.\n",
    "\n",
    "**Note:** You might need to go back and forth during your implementation of the code. The structure is set up to make implementation easier, but how you return values from the different functions might vary, and you might find yourself going back and change something to make it easier later on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment preparations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We help you out with importing the libraries.\n",
    "\n",
    "**IMPORTANT NOTE:** You may not import any more libraries than the ones already imported!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision tree model\n",
    "\n",
    "The main objective is to implement the decision tree model. The implemented decision tree needs to be recursive model, that is, it should be implemented general enough to call itself in order to grow. \"Growing\" a tree refers to the same thing as \"training\" a model.\n",
    "\n",
    "As said in the introduction, the structure is set up to help with implementation, but the nature of this model makes it a bit harder to implement function-by-function. You will most likely go back and forth between these first tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Grow Tree\n",
    "\n",
    "We will start with the main function of the decision tree, the \"growing\" function. \n",
    "\n",
    "This function should be called when creating the model, but also from inside itself. It is responible for creating all the nodes and leafs in the tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_appropriate_label(data):\n",
    "    label_values = np.unique(data)\n",
    "    label_app_perc = np.zeros(len(label_values))\n",
    "    for label in data:\n",
    "        pos_idx = np.where(label_values == label)[0][0]\n",
    "        label_app_perc[pos_idx] += 1\n",
    "    label_app_perc /= len(data)\n",
    "    return int(label_values[np.argmax(label_app_perc)]), np.max(label_app_perc)\n",
    "\n",
    "def check_if_homogeneous(data, threshold = 0.8):\n",
    "    if most_appropriate_label(data)[1] >= threshold:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def best_feat_splitting_point(best_feat_idx, split_thr, feature_set):\n",
    "    best_feat = np.sort(extract_feat(best_feat_idx, feature_set))\n",
    "    return (best_feat[split_thr - 1] + best_feat[split_thr]) / 2\n",
    "\n",
    "def grow_tree(data, feature_set, feature_names, target_name, gini_instead_of_entropy = False):\n",
    "\n",
    "    # TODO: Implement the rest of this function.\n",
    "    # NOTE: You will need to come back to this function to make use of later functions.\n",
    "\n",
    "    if check_if_homogeneous(data):\n",
    "        return {\"type\": \"leaf\", \"label\": most_appropriate_label(data)[0]}\n",
    "    best_feat_idx, split_thr, subset_l_feats, subset_r_feats, subset_l_labels, subset_r_labels = split_data(data, feature_set, gini_instead_of_entropy)\n",
    "    splitting_point = best_feat_splitting_point(best_feat_idx, split_thr, feature_set)\n",
    "    node = {\"type\": \"node\", \"feature\": feature_names[best_feat_idx], \"feature_index\": best_feat_idx, \"splitting_point\": splitting_point}\n",
    "    if subset_l_labels.shape[0] != 0:\n",
    "        node[\"left\"] = grow_tree(subset_l_labels, subset_l_feats, feature_names, target_name, gini_instead_of_entropy)\n",
    "    else:\n",
    "        node[\"left\"] = {\"type\": \"leaf\", \"label\": most_appropriate_label(data)[0]}\n",
    "    if subset_r_labels.shape[0] != 0:\n",
    "        node[\"right\"] = grow_tree(subset_r_labels, subset_r_feats, feature_names, target_name, gini_instead_of_entropy)\n",
    "    else:\n",
    "        node[\"right\"] = {\"type\": \"leaf\", \"label\": most_appropriate_label(data)[0]}\n",
    "    return node "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Growth stopping conditions (or stopping criterias)\n",
    "\n",
    "The \"grow_tree\" function needs some way of stop growing, otherwise it will grow indefinitely. We will adress this issue here.\n",
    "\n",
    "The trees stopping criterias needs to handle the following:\n",
    "\n",
    "1) When a node has only datapoints of a single class.\n",
    "\n",
    "2) Prevent the tree from growing to large, i.e., a max depth.\n",
    "\n",
    "3) Prevent the tree nodes from becoming to small.\n",
    "\n",
    "4) Prevent the tree from growing when the node is large (has a lot of datapoints) but it is very unbalanced. This is an extention to case 1.\n",
    "\n",
    "Can you think of some other stopping criterias that is good to have? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Change the name of the functions and implement them as you see fit.\n",
    "\n",
    "def stop1(...):\n",
    "\n",
    "    pass \n",
    "\n",
    "def stop2(...):\n",
    "\n",
    "    pass \n",
    "\n",
    "\n",
    "... # Add more stopping criterias if needed. Don't forget to use them when growing the tree!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Best feature for splitting nodes\n",
    "\n",
    "When we are growing the tree, we need to decide how we are going to split a node into two new nodes. This is achived by looking at the features of the data in the node and calculate the best feature to split on.\n",
    "\n",
    "Here you have a choice:\n",
    "\n",
    "* Split using **Information Entropy**\n",
    "* Split using **Gini Impurity**\n",
    "\n",
    "Finish the function below using Information Entropy or Gini Impurity.\n",
    "\n",
    "**Note:** Your code should be able to handle both real and categorical features!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_probability(value, data):\n",
    "    pb_value = np.where(data == value)[0].shape[0] / data.shape[0]\n",
    "    return pb_value\n",
    "\n",
    "def entropy(data):\n",
    "    ent = 0\n",
    "    for value in np.unique(data):\n",
    "        ent += calculate_probability(value, data) * np.log(calculate_probability(value, data))\n",
    "    ent = -ent\n",
    "    return ent\n",
    "\n",
    "def gini_index(data):\n",
    "    gini = 1\n",
    "    for value in np.unique(data):\n",
    "        gini -= calculate_probability(value, data)**2\n",
    "    return gini\n",
    "\n",
    "# TODO: Change the name to which ever you chose.\n",
    "def best_split_feature(data, feature_set, imp_msr = True):  # true for \"Entropy\", false for \"Gini\"\n",
    "\n",
    "    # TODO: Implement the rest of this function.\n",
    "    # NOTE: Do not forget to have support for real and categorical features.\n",
    "    split_thr = 1\n",
    "    min_imp = 1\n",
    "    for i in range(feature_set.shape[1]):\n",
    "        feature = np.array([])\n",
    "        for features in feature_set:\n",
    "            feature = np.append(feature, float(features[i]))\n",
    "        index_order = np.argsort(feature)\n",
    "        for threshold in range(1, len(feature)):\n",
    "            subset_l_pos, subset_r_pos = index_order[threshold:], index_order[:threshold]\n",
    "            subset_l, subset_r = np.asarray([]), np.asarray([])\n",
    "            for pos in subset_l_pos:\n",
    "                subset_l = np.append(subset_l, int(data[pos]))\n",
    "            for pos in subset_r_pos:\n",
    "                subset_r = np.append(subset_r, int(data[pos]))\n",
    "            if imp_msr:\n",
    "                imp = (subset_l.shape[0] * entropy(subset_l) + subset_r.shape[0] * entropy(subset_r)) / data.shape[0]\n",
    "            else:\n",
    "                imp = (subset_l.shape[0] * gini_index(subset_l) + subset_r.shape[0] * gini_index(subset_r)) / data.shape[0]\n",
    "            if imp < min_imp:\n",
    "                min_imp = imp\n",
    "                best_feat_idx = i\n",
    "                split_thr = threshold\n",
    "\n",
    "    # need to provide support for categorical features\n",
    "    # ...\n",
    "    \n",
    "    return best_feat_idx, split_thr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Split data\n",
    "\n",
    "When growing the tree, we need to split the data multiple times, and what we decide to split varies a lot. It is similar to splitting data into train and test sets (remember from assignment 1), but we split the data based on the best feature for growing a good tree.\n",
    "\n",
    "**IMPORTANT NOTE:** To calculate binary splits for real-valued features, the following rule must be applied: an instance with a feature value lower than the mean feature value follows the left edge from the split node while all other instances follow the right edge from the split node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_feat(feat_idx, feature_set):\n",
    "    feature = np.array([])\n",
    "    for features in feature_set:\n",
    "        feature = np.append(feature, float(features[feat_idx]))\n",
    "    return feature\n",
    "\n",
    "def extract_subsets(data, feature_set, feature, threshold):\n",
    "    index_order = np.argsort(feature)\n",
    "    subset_l_pos, subset_r_pos = index_order[:threshold], index_order[threshold:]\n",
    "    subset_l_feats, subset_r_feats, subset_l_labels, subset_r_labels = np.asarray([]), np.asarray([]), np.asarray([]), np.asarray([])\n",
    "    for pos in subset_l_pos:\n",
    "        subset_l_feats = np.append(subset_l_feats, feature_set[pos], axis=0)\n",
    "        subset_l_labels = np.append(subset_l_labels, int(data[pos]))\n",
    "    for pos in subset_r_pos:\n",
    "        subset_r_feats = np.append(subset_r_feats, feature_set[pos], axis=0)\n",
    "        subset_r_labels = np.append(subset_r_labels, int(data[pos]))\n",
    "    return subset_l_feats.reshape(len(subset_l_pos), feature_set.shape[1]).astype(float), subset_r_feats.reshape(len(subset_r_pos), feature_set.shape[1]).astype(float), subset_l_labels, subset_r_labels\n",
    "\n",
    "def split_data(data, feature_set, gini_instead_of_entropy):\n",
    "\n",
    "    # TODO: Implement the rest of this function.\n",
    "    \n",
    "    best_feat_idx, split_thr = best_split_feature(data, feature_set, gini_instead_of_entropy)\n",
    "    best_feat = extract_feat(best_feat_idx, feature_set)\n",
    "    subset_l_feats, subset_r_feats, subset_l_labels, subset_r_labels = extract_subsets(data, feature_set, best_feat, split_thr)\n",
    "    return best_feat_idx, split_thr, subset_l_feats, subset_r_feats, subset_l_labels, subset_r_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Predict with tree model\n",
    "\n",
    "Finally, when we have grown our tree, we would like to use it for prediction. When using the tree for prediction, we traverse the tree for each datapoint untill we land in a leaf node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_element(tree, test_element):\n",
    "    if tree[\"type\"] == \"leaf\":\n",
    "        return tree[\"label\"]\n",
    "    elif tree[\"type\"] == \"node\":\n",
    "        if float(test_element[tree[\"feature_index\"]]) <= float(tree[\"splitting_point\"]):\n",
    "            return predict_element(tree[\"left\"], test_element)\n",
    "        else:\n",
    "            return predict_element(tree[\"right\"], test_element)\n",
    "\n",
    "def predict_with_tree(tree, test_set):\n",
    "\n",
    "    # TODO: Implement the rest of this function.\n",
    "    # NOTE: This function should also be recursive.\n",
    "\n",
    "    predicted_result = np.zeros(test_set.shape[0]).astype(\"str\")\n",
    "    for i in range(test_set.shape[0]):\n",
    "        predicted_result[i] = predict_element(tree, test_set[i])\n",
    "    return predicted_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test decision tree model, compare with scikit learn, and plot dataset results\n",
    "\n",
    "In the last part of the lab, you are going to test your tree code and compare it to scikit learn. The goal is not to be better than an established library, but to give you an indication about if you are on the right track.\n",
    "\n",
    "You will need to plot the results from your model and the scikit learn model using matplotlib. We suggest a simple but informative bar-charts.\n",
    "\n",
    "To make the comparison fair, you should train and test both your decision tree algorithm and the scikit learn at least 5 times, and shuffle the data each time before splitting the data into a train and test set.\n",
    "\n",
    "The datasets are:\n",
    "\n",
    "* Wine - (https://archive.ics.uci.edu/dataset/109/wine)\n",
    "* Heart disease - (https://archive.ics.uci.edu/dataset/45/heart+disease)\n",
    "* Car - (https://archive.ics.uci.edu/dataset/19/car+evaluation)\n",
    "\n",
    "**IMPORTANT NOTE 1:** Take note of the feature types in the datasets, some features are numerical in value but are in fact categorical features. Be sure to handle these features correctly!\n",
    "\n",
    "**IMPORTANT NOTE 2:** In this assignment it helps to add an additional header with information about the features and if they are nominal (n) or real (r) features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may use the \"**accuracy_score**\" function from scikit learn (imported above) to compare the performance of your own and scikit learns models.\n",
    "\n",
    "See below for an example use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = [1,1,1,1,1] # Pretend labels\n",
    "y_pred = [1,1,2,2,1] # Pretend prediction\n",
    "\n",
    "accuracy_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6) Dataset 1: Wine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITERATION 0\n",
      "implemented tree model accuracy: 0.8444444444444444\n",
      "scikit tree model accuracy: 0.8444444444444444\n",
      "ITERATION 1\n",
      "implemented tree model accuracy: 0.8666666666666667\n",
      "scikit tree model accuracy: 0.8444444444444444\n",
      "ITERATION 2\n",
      "implemented tree model accuracy: 0.8\n",
      "scikit tree model accuracy: 0.9333333333333333\n",
      "ITERATION 3\n",
      "implemented tree model accuracy: 0.8666666666666667\n",
      "scikit tree model accuracy: 0.9555555555555556\n",
      "ITERATION 4\n",
      "implemented tree model accuracy: 0.8\n",
      "scikit tree model accuracy: 0.9333333333333333\n",
      "ITERATION 5\n",
      "implemented tree model accuracy: 0.8666666666666667\n",
      "scikit tree model accuracy: 0.9555555555555556\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_wine = pd.read_csv(\"wine.csv\").to_numpy()\n",
    "\n",
    "# TODO: Set up the data and split it into train and test-sets.\n",
    "data_wine_feats_train, data_wine_feats_test, data_wine_labels_train, data_wine_labels_test = train_test_split(data_wine[1:, 0:-1], data_wine[1:, -1], random_state=0)\n",
    "data_wine_feature_names, data_wine_target_name = pd.read_csv(\"wine.csv\", nrows=0).columns.to_numpy()[0:-1], pd.read_csv(\"wine.csv\", nrows=0).columns.to_numpy()[-1]\n",
    "\n",
    "model_accuracies = np.array([])\n",
    "scikit_accuracies = np.array([])\n",
    "\n",
    "# TODO: Train and test your implemented tree model.\n",
    "# NOTE: Use the same train/test split for your tree model and the scikit learn model\n",
    "def calculate_accuracy_model(train_feats, test_feats, train_labels, test_labels, feature_names, target_name):\n",
    "    tree = grow_tree(train_labels, train_feats, feature_names, target_name)\n",
    "    predicted_test_set_labels = predict_with_tree(tree, test_feats)\n",
    "    actual_test_set_labels = test_labels\n",
    "    data_wine_accuracy_model = accuracy_score(actual_test_set_labels, predicted_test_set_labels)\n",
    "    return data_wine_accuracy_model\n",
    "\n",
    "# TODO: Train and test scikit learns tree model.\n",
    "# NOTE: Use the same train/test split for your tree model and the scikit learn model\n",
    "def calculate_accuracy_scikit(train_feats, test_feats, train_labels, test_labels):\n",
    "    dtc = DecisionTreeClassifier()\n",
    "    dtc.fit(train_feats, train_labels)\n",
    "    data_wine_labels_pred = dtc.predict(test_feats)\n",
    "    data_wine_accuracy_sk = accuracy_score(test_labels, data_wine_labels_pred)\n",
    "    return data_wine_accuracy_sk\n",
    "\n",
    "def print_accuracy_both_models(train_feats, test_feats, train_labels, test_labels, feature_names, target_names, model_accuracies, scikit_accuracies):\n",
    "    model_accuracy = calculate_accuracy_model(train_feats, test_feats, train_labels, test_labels, feature_names, target_names)\n",
    "    model_accuracies = np.append(model_accuracies, model_accuracy)\n",
    "    scikit_accuracy = calculate_accuracy_scikit(train_feats, test_feats, train_labels, test_labels)\n",
    "    scikit_accuracies = np.append(scikit_accuracies, scikit_accuracy)\n",
    "    print(f\"ITERATION {len(model_accuracies) - 1}\")\n",
    "    print(f'implemented tree model accuracy: {model_accuracy}')\n",
    "    print(f'scikit tree model accuracy: {scikit_accuracy}')\n",
    "    return model_accuracies, scikit_accuracies\n",
    "\n",
    "model_accuracies, scikit_accuracies = print_accuracy_both_models(data_wine_feats_train, data_wine_feats_test, data_wine_labels_train, data_wine_labels_test, data_wine_feature_names, data_wine_target_name, model_accuracies, scikit_accuracies)\n",
    "\n",
    "# TODO: Do the above at least 5 times\n",
    "# NOTE: Use loops here!\n",
    "for i in range(5):\n",
    "    data_wine_feats_train, data_wine_feats_test, data_wine_labels_train, data_wine_labels_test = train_test_split(data_wine[1:, 0:-1], data_wine[1:, -1])\n",
    "    model_accuracies, scikit_accuracies = print_accuracy_both_models(data_wine_feats_train, data_wine_feats_test, data_wine_labels_train, data_wine_labels_test, data_wine_feature_names, data_wine_target_name, model_accuracies, scikit_accuracies)\n",
    "\n",
    "# TODO: Plot the results with matplotlib (plt)\n",
    "# NOTE: One plot with all results is enough\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "x = np.arange(len(model_accuracies))\n",
    "w = 0.4\n",
    "\n",
    "ax.bar(x - w/2, model_accuracies, width=w, label=\"Model Accuracy\")\n",
    "ax.bar(x + w/2, scikit_accuracies, width=w, label=\"Scikit Accuracy\")\n",
    "\n",
    "ax.set_ylim(0, 1)\n",
    "ax.set_xlabel(\"Iterations\")\n",
    "ax.set_ylabel(\"Accuracy\")\n",
    "ax.legend(loc=\"lower left\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7) Dataset 2: Heart Disease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_heart = pd.read_csv(\"heart.csv\").to_numpy()\n",
    "\n",
    "# TODO: Set up the data and split it into train and test-sets.\n",
    "\n",
    "# TODO: Train and test your implemented tree model.\n",
    "# NOTE: Use the same train/test split for your tree model and the scikit learn model\n",
    "\n",
    "# TODO: Train and test scikit learns tree model.\n",
    "# NOTE: Use the same train/test split for your tree model and the scikit learn model\n",
    "\n",
    "# TODO: Do the above at least 5 times\n",
    "# NOTE: Use loops here!\n",
    "\n",
    "# TODO: Plot the results with matplotlib (plt)\n",
    "# NOTE: One plot with all results is enough"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8) Dataset 3: Car"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_car = pd.read_csv(\"car.csv\").to_numpy()\n",
    "\n",
    "# TODO: Set up the data and split it into train and test-sets.\n",
    "\n",
    "# TODO: Train and test your implemented tree model.\n",
    "# NOTE: Use the same train/test split for your tree model and the scikit learn model\n",
    "\n",
    "# TODO: Train and test scikit learns tree model.\n",
    "# NOTE: Use the same train/test split for your tree model and the scikit learn model\n",
    "\n",
    "# TODO: Do the above at least 5 times\n",
    "# NOTE: Use loops here!\n",
    "\n",
    "# TODO: Plot the results with matplotlib (plt)\n",
    "# NOTE: One plot with all results is enough"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9) Training with normalized data on the wine-dataset\n",
    "\n",
    "So far, we have trained our decision trees with \"raw\" data, i.e., we haven't done much preprocessing on the data.\n",
    "\n",
    "Here we will do minor preprocessing on the data with the help of the scikit-learn library: https://scikit-learn.org/stable/modules/preprocessing.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "# TODO: Use the wine dataset from above and scale its features (Not the labels!) between 0 and 1\n",
    "\n",
    "# TODO: Run the code from the dataset and compare the preprocessed vs non-preprocessed data.\n",
    "# NOTE: You can copy most of the workflow from the dataset code above to save you some time.\n",
    "# NOTE: Use the same train/test split for your tree model for both the preprocessed vs non-preprocessed data.\n",
    "\n",
    "# TODO: Do the above at least 5 times\n",
    "# NOTE: Use loops here!\n",
    "\n",
    "# TODO: Plot the results with matplotlib (plt)\n",
    "# NOTE: One plot with all results is enough"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions for examination:\n",
    "\n",
    "In addition to completing the assignment with all its tasks, you should also prepare to answer the following questions:\n",
    "\n",
    "1) Why is growing the tree indefinitely such a bad idea? The performance would increase would it not?\n",
    "\n",
    "2) Beside preventing the tree from growing to large, what is the purpose of 'stopping criterias'?\n",
    "\n",
    "3) What is the difference between **Information Entropy** and **Gini Impurity**?\n",
    "\n",
    "4) What are some pros about using decision trees?\n",
    "\n",
    "5) Did preprocessing the data help with performance when using decision trees?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finished!\n",
    "\n",
    "Was part of the setup incorrect? Did you spot any inconsistencies in the assignment? Could something improve?\n",
    "\n",
    "If so, please write them and send via email and send it to:\n",
    "\n",
    "* marcus.gullstrand@ju.se\n",
    "\n",
    "Thank you!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
